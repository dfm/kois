#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import sys
import kplr
import sqlite3
import logging
import numpy as np
import cPickle as pickle
from datetime import datetime
import matplotlib.pyplot as pl
from ConfigParser import ConfigParser

try:
    import kois
    kois = kois
except ImportError:
    sys.path.insert(0, os.path.dirname(os.path.dirname(
        os.path.abspath(__file__))))
    import kois
    kois = kois

from kois import build_model

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Build a KOI model")
    parser.add_argument("koi_id", help="The KOI number")
    parser.add_argument("config", help="Path to the configuration file")
    parser.add_argument("--window_factor", default=4, type=float,
                        help="The number of transit durations in the data "
                             "window")
    parser.add_argument("--detrend_factor", default=1.1, type=float,
                        help="The number of transit durations to mask when "
                             "de-trending")
    parser.add_argument("--poly_order", default=5, type=int,
                        help="The order of polynomial to use for de-trending")
    parser.add_argument("--plots", action="store_true",
                        help="Make some basic figures")
    parser.add_argument("--submit", action="store_true",
                        help="Submit the job to the cluster")
    args = parser.parse_args()
    logging.basicConfig(level=logging.INFO)
    koi_id = args.koi_id

    model = build_model(koi_id, window_factor=args.window_factor,
                        detrend_factor=args.detrend_factor,
                        poly_order=args.poly_order)

    # Parse the configuration file.
    config = ConfigParser()
    config.read(args.config)

    # Set up the output directory.
    outdir = config.get("local", "basepath")
    outdir = os.path.join(outdir, "{0}".format(koi_id))

    try:
        os.makedirs(outdir)
    except os.error:
        logging.info("Output directory '{0}' exists".format(outdir))

    # Save the initial model.
    pickle.dump(model, open(os.path.join(outdir, "model.pkl"), "wb"), -1)
    ndim = len(model.vector)

    min_nodes = int(config.get("remote", "min_nodes"))
    ppn = int(config.get("remote", "ppn"))

    nodes = int(max(np.ceil(ndim/ppn), min_nodes))
    nwalkers = 2 * ppn * nodes
    logging.info("Sampling {0} parameters with {1} walkers".format(ndim,
                                                                   nwalkers))
    logging.info("    [{0} planet(s)]".format(len(model.periods)))
    logging.info("Initial lnprob: {0}".format(model.lnprob()))

    cpus = nodes * ppn
    script = open(config.get("local", "pbs_template")).read()
    script = script.format(nodes=nodes, cpus=cpus, koi_id=koi_id,
                           nwalkers=nwalkers)

    with open(os.path.join(outdir, "job.pbs"), "w") as f:
        f.write(script)

    # Fist submit and exit if asked.
    if args.submit:
        import subprocess

        # Make sure that the remote directory exists.
        cmd = ("ssh {0} \"mkdir -p {1}\""
               .format(config.get("remote", "hostname"),
                       os.path.join(config.get("remote", "basepath"),
                                    koi_id)))
        logging.info("Running command:")
        logging.info("    {0}".format(cmd))
        subprocess.check_call(cmd, shell=True)

        # Copy the precomputed model.
        files = map(lambda f: os.path.join(outdir, f),
                    ["model.pkl", "job.pbs"])
        remote_path = os.path.join(config.get("remote", "basepath"), koi_id)
        cmd = ("scp -r {0} {1}:{2}"
               .format(" ".join(files),
                       config.get("remote", "hostname"), remote_path))
        logging.info("Running command:")
        logging.info("    {0}".format(cmd))
        subprocess.check_call(cmd, shell=True)

        # Queue the job.
        cmd = ("ssh {0} \"qsub {1}/job.pbs\""
               .format(config.get("remote", "hostname"), remote_path))
        logging.info("Running command:")
        logging.info("    {0}".format(cmd))
        remote_id = subprocess.check_output(cmd, shell=True)

        # Update the database.
        with sqlite3.connect(config.get("local", "database")) as conn:
            c = conn.cursor()
            c.execute("update kois set submitted=?, remote_id=? "
                      "where kepoi_name=?",
                      (datetime.now(), remote_id, koi_id))

    logging.info("Generating folded light curve plots with initial model")

    nplanets = len(model.periods)

    P, t0, dur = model.periods[0], model.epochs[0], model.durations[0]
    fig, axes = pl.subplots(2, 1, figsize=(6, 6),
                            sharex=True)
    fig.subplots_adjust(left=0.17, bottom=0.1, right=0.9,
                        top=0.9, wspace=0.05, hspace=0.05)

    # Plot the folded datasets.
    hp = 0.5 * P
    for d in model.datasets:
        ax = axes[0]
        if d.texp < 0.01:
            ax = axes[1]
        ax.plot((d.time-t0+hp) % P - hp, d.flux, ".k", alpha=0.3, ms=3)

    # Plot the models.
    t = np.linspace(-4*dur, 4*dur, 1000)
    lc = kplr.EXPOSURE_TIMES[1]/86400.0
    axes[0].plot(t, model.get_light_curve(t+t0, texp=lc), "r", lw=1)
    sc = kplr.EXPOSURE_TIMES[0]/86400.0
    axes[1].plot(t, model.get_light_curve(t+t0, K=3, texp=sc), "r", lw=2)

    # Set the y-axis limits.
    for ax in axes:
        ylim = min(ax.get_ylim()[0] - 1, -1e-3)
        ax.set_ylim(1+1.2*ylim, 1-0.6*ylim)

    # Labels.
    axes[0].set_xlim(-4*dur, 4*dur)
    axes[0].set_title("KOI {0}, period = {1} days".format(koi_id, P))
    axes[1].set_xlabel("time since transit")
    fig.savefig(os.path.join(outdir, "light-curve.png"))
