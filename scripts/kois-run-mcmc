#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import sys
import h5py
import emcee
import logging
import numpy as np
import cPickle as pickle
from emcee.utils import MPIPool

try:
    import kois
    kois = kois
except ImportError:
    sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    import kois
    kois = kois

import argparse
parser = argparse.ArgumentParser(description="Build a KOI model")
parser.add_argument("model", help="The path to the model file")
parser.add_argument("-n", "--nsteps", default=50000, type=int,
                    help="The number of steps of MCMC to run")
parser.add_argument("-b", "--burnin", default=2000, type=int,
                    help="The number of burn-in steps to run")
parser.add_argument("-w", "--nwalkers", default=None, type=int,
                    help="The number of walkers to use")
parser.add_argument("-o", "--outdir", default=None,
                    help="The directory where the output files will be "
                    "written")
parser.add_argument("-r", "--restart", action="store_true",
                    help="Restart sampling from a previously run chain")
args = parser.parse_args()

logging.basicConfig(level=logging.INFO)

# Set up the output directory.
outdir = args.outdir
if outdir is None:
    outdir = os.path.dirname(os.path.abspath(args.model))
try:
    os.makedirs(outdir)
except os.error:
    logging.info("Output directory '{0}' exists".format(outdir))

# Load the pickled model.
logging.info("Loading the precomputed model.")
model = pickle.load(open(args.model))

# Define the log-probability function.
def lnprobfn(p):
    return model(p)

# Wait for instructions if not master.
pool = MPIPool()
if not pool.is_master():
    pool.wait()
    sys.exit(0)

# Choose the number of walkers.
ndim = len(model.vector)
if args.nwalkers is None:
    nwalkers = int(max(np.ceil(ndim/12.0), 2)*24)
    # nwalkers = int(2 ** np.ceil(np.log2(ndim) + 1))
else:
    nwalkers = args.nwalkers
logging.info("Sampling {0} parameters with {1} walkers".format(ndim,
                                                               nwalkers))

# Load positions if restarting.
results_filename = os.path.join(outdir, "mcmc.h5")
if args.restart:
    logging.info("Restarting")
    with h5py.File(results_filename, "r") as f:
        iteration0 = int(f.attrs["iteration"]) + 1
        p0 = np.array(f["samples"][:, iteration0, :])
    nwalkers, ndim = p0.shape
    assert ndim == len(model.vector), "Dimension mismatch"

else:
    # Overwrite any existing results file.
    n = len(model.periods)
    with h5py.File(results_filename, "w") as f:
        f.create_dataset("samples", (nwalkers, args.nsteps, len(model.vector)),
                         "f")
        f.create_dataset("lnprob", (nwalkers, args.nsteps), "f")

    # Choose some random positions.
    iteration0 = 0
    p0 = [model.vector + 1e-6 * np.random.randn(ndim) for i in range(nwalkers)]

# Set up the sampler.
sampler = emcee.EnsembleSampler(nwalkers, ndim, lnprobfn, pool=pool)

if not args.restart:
    # Run a burn-in.
    print("Running burn-in")
    sampler.run_mcmc(p0, args.burnin)

    # Re-sample the walkers close to the best sample seen yet.
    lp = sampler.lnprobability
    k, it = np.unravel_index(np.argmax(lp), lp.shape)
    v = sampler.chain[k, it, :]
    p0 = [v + 1e-6 * np.random.randn(ndim) for i in range(nwalkers)]

    # Clear the burn-in.
    sampler.reset()

# Do the sampling.
for i, (pos, lp, state) in enumerate(sampler.sample(p0, iterations=args.nsteps,
                                                    storechain=False)):
    print(i)
    with h5py.File(results_filename) as f:
        f.attrs["iteration"] = iteration0+i
        f["samples"][:, i, :] = pos
        f["lnprob"][:, i] = lp

pool.close()
