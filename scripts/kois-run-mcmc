#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import sys
import h5py
import emcee
import logging
import numpy as np
import cPickle as pickle
from emcee.utils import MPIPool

try:
    import kois
    kois = kois
except ImportError:
    sys.path.insert(0, os.path.dirname(os.path.dirname(
        os.path.abspath(__file__))))
    import kois
    kois = kois

import argparse
parser = argparse.ArgumentParser(description="Build a KOI model")
parser.add_argument("model", help="The path to the model file")
parser.add_argument("-n", "--nsteps", default=40000, type=int,
                    help="The number of steps of MCMC to run")
parser.add_argument("-f", "--factor", default=20000, type=int,
                    help="The number of extra steps per planet")
parser.add_argument("-b", "--burnin", default=10000, type=int,
                    help="The number of burn-in steps to run")
parser.add_argument("-w", "--nwalkers", default=None, type=int,
                    help="The number of walkers to use")
parser.add_argument("-o", "--outdir", default=None,
                    help="The directory where the output files will be "
                    "written")
parser.add_argument("-r", "--restart", action="store_true",
                    help="Restart sampling from a previously run chain")
args = parser.parse_args()

logging.basicConfig(level=logging.INFO)

# Set up the output directory.
outdir = args.outdir
if outdir is None:
    outdir = os.path.dirname(os.path.abspath(args.model))
try:
    os.makedirs(outdir)
except os.error:
    logging.info("Output directory '{0}' exists".format(outdir))

# Load the pickled model.
logging.info("Loading the precomputed model.")
model = pickle.load(open(args.model))

# Define the log-probability function.
def lnprobfn(p):
    return model(p)

# Wait for instructions if not master.
pool = MPIPool()
if not pool.is_master():
    pool.wait()
    sys.exit(0)

# Choose the number of walkers.
ndim = len(model.vector)
if args.nwalkers is None:
    nwalkers = int(max(np.ceil(ndim/12.0), 2)*24)
else:
    nwalkers = args.nwalkers
logging.info("Sampling {0} parameters with {1} walkers".format(ndim,
                                                               nwalkers))

# Compute the total number of steps to take.
ntot = args.nsteps + args.factor * len(model.periods)

# Load positions if restarting.
results_filename = os.path.join(outdir, "mcmc.h5")
initial_vector = np.array(model.vector)
if args.restart:
    logging.info("Restarting")
    with h5py.File(results_filename) as f:
        iteration0 = int(f.attrs["iteration"]) + 1
        p0 = np.array(f["samples"][:, iteration0, :])
        f["samples"].resize(iteration0+ntot, axis=1)
        f["lnprob"].resize(iteration0+ntot, axis=1)
    nwalkers, ndim = p0.shape
    assert ndim == len(model.vector), "Dimension mismatch"

else:
    # Overwrite any existing results file.
    n = len(model.periods)
    with h5py.File(results_filename, "w") as f:
        f.create_dataset("samples", (nwalkers, ntot, len(model.vector)),
                         np.float64)
        f.create_dataset("lnprob", (nwalkers, ntot), np.float64)

    # Choose some random positions.
    iteration0 = 0
    p0 = []
    for i in range(nwalkers):
        flag = True
        for j in xrange(5000):
            v = initial_vector + 1e-6 * np.random.randn(ndim)
            model.vector = v
            if np.isfinite(model.lnprob()):
                p0.append(v)
                flag = False
                break
        if flag:
            raise RuntimeError("Couldn't find reasonable starting position.")

# Set up the sampler.
sampler = emcee.EnsembleSampler(nwalkers, ndim, lnprobfn, pool=pool)

if not args.restart:
    # Run a burn-in.
    print("Running burn-in")
    sampler.run_mcmc(p0, args.burnin)

    # Re-sample the walkers close to the best sample seen yet.
    lp = sampler.lnprobability
    k, it = np.unravel_index(np.argmax(lp), lp.shape)
    initial_vector = sampler.chain[k, it, :]
    p0 = []
    for i in range(nwalkers):
        flag = True
        for j in xrange(5000):
            v = initial_vector + 1e-6 * np.random.randn(ndim)
            model.vector = v
            if np.isfinite(model.lnprob()):
                p0.append(v)
                flag = False
                break
        if flag:
            raise RuntimeError("Couldn't find position post-burn-in.")

    # Clear the burn-in.
    sampler.reset()

# Do the sampling.
batch_size = 500
pos_tmp = np.empty((nwalkers, batch_size, ndim))
lp_tmp = np.empty((nwalkers, batch_size))
for i, (pos, lp, state) in enumerate(sampler.sample(p0, iterations=ntot,
                                                    storechain=False)):
    j = i % batch_size
    pos_tmp[:, j, :] = pos
    lp_tmp[:, j] = lp
    if j == batch_size - 1:
        print(i, np.mean(sampler.acceptance_fraction))
        with h5py.File(results_filename) as f:
            f.attrs["iteration"] = iteration0+i
            f["samples"][:, i-batch_size+1:i+1, :] = pos_tmp
            f["lnprob"][:, i-batch_size+1:i+1] = lp_tmp

pool.close()
